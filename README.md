# ğŸ™ï¸ Chatterbox TTS with NVIDIA Triton Inference Server

This repository demonstrates how to deploy and run the [ResembleAI Chatterbox TTS model](https://huggingface.co/ResembleAI/chatterbox) using the NVIDIA Triton Inference Server and run real-time inference using the Triton HTTP client.

---

## ğŸš€ Features

- âœ… Serve Chatterbox TTS model locally with Triton Inference Server
- âœ… Load weights from `.safetensors` (no HuggingFace download)
- âœ… Send HTTP requests to Triton with Triton Python Client
- âœ… Convert and play model output as `.wav` audio
- âœ… Fully Docker-compatible setup

---

## ğŸ§° Folder Structure

model_repository/
â””â”€â”€ src/
    â””â”€â”€ chatterbox/
        â”œâ”€â”€ 1/
            â”‚ â””â”€â”€ model.py # Python backend model
        â””â”€â”€ config.pbtxt
    â””â”€â”€ weights/
    â””â”€â”€ Dockerfile
    â””â”€â”€ requirements.txt
    â””â”€â”€ test/
        â””â”€â”€ test_client.py
---

## âš™ï¸ Prerequisites

- Python 3.12+
- CUDA-enabled GPU
- Triton Inference Server
- Docker (optional but recommended)

### ğŸ”§ Build and Deploy

```bash
docker build -t triton-chatterbox .
```

ğŸ§  Run Triton Server

```bash
docker run --rm --gpus=1 \
  -v $(pwd)/src:/app/models \
  -p8000:8000 -p8001:8001 -p8002:8002 \
  triton-chatterbox
```

### ğŸ”§ Install Dependencies and Testing

```bash
pip install tritonclient[http] soundfile numpy torch
python3 test_client.py
```







